{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MEG_IED_Detector.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyMkddHd5847lQKkOIFP/bKt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jenny12138/CNN_SinglePatient/blob/main/MEG_IED_Detector.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 1: Set Up and Load All Data from Drive\n"
      ],
      "metadata": {
        "id": "Orh99_luJSpV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D8OGw_-0JN00"
      },
      "outputs": [],
      "source": [
        "## Import the necessary packages/modules/libraries\n",
        "\n",
        "import copy\n",
        "import scipy.io\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt \n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
        "from tensorflow.keras import metrics\n",
        "from tensorflow import keras\n",
        "from sklearn.utils import shuffle\n",
        "import collections\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import train_test_split \n",
        "import pickle\n",
        "from sklearn.preprocessing import MinMaxScaler"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Mount Google Drive to this notebook so that we can import the processed EEG data\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "gwcPv2KuJjW5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Import positive data (epileptiform epochs) from MATLAB files\n",
        "\n",
        "# Import from run 3\n",
        "EST_run03_matlab = scipy.io.loadmat('/content/drive/MyDrive/Pt0090_Data_MEGCleaned/EST_03.mat')\n",
        "EST_run03_F = EST_run03_matlab['EST_03F']\n",
        "\n",
        "# Import from run 4\n",
        "EST_run04_matlab = scipy.io.loadmat('/content/drive/MyDrive/Pt0090_Data_MEGCleaned/EST_04.mat')\n",
        "EST_run04_F = EST_run04_matlab['EST_04F']\n",
        "\n",
        "# Import from run 5\n",
        "EST_run05_matlab = scipy.io.loadmat('/content/drive/MyDrive/Pt0090_Data_MEGCleaned/EST_05.mat')\n",
        "EST_run05_F = EST_run05_matlab['EST_05F']\n",
        "\n",
        "# Import from run 6\n",
        "EST_run06_matlab = scipy.io.loadmat('/content/drive/MyDrive/Pt0090_Data_MEGCleaned/EST_06.mat')\n",
        "EST_run06_F = EST_run06_matlab['EST_06F']"
      ],
      "metadata": {
        "id": "RpL0zRVoJs6E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Import negative data (non-epileptiform epochs) from MATLAB files\n",
        "\n",
        "# Import from run 3\n",
        "negative_run03_matlab = scipy.io.loadmat('/content/drive/MyDrive/Pt0090_Data_MEGCleaned/NEG_03.mat')\n",
        "negative_run03_F = negative_run03_matlab['NEG_03F']\n",
        "\n",
        "# Import from run 4\n",
        "negative_run04_matlab = scipy.io.loadmat('/content/drive/MyDrive/Pt0090_Data_MEGCleaned/NEG_04.mat')\n",
        "negative_run04_F = negative_run04_matlab['NEG_04F']\n",
        "\n",
        "# Import from run 5\n",
        "negative_run05_matlab = scipy.io.loadmat('/content/drive/MyDrive/Pt0090_Data_MEGCleaned/NEG_05.mat')\n",
        "negative_run05_F = negative_run05_matlab['NEG_05F']\n",
        "\n",
        "# Import from run 6\n",
        "negative_run06_matlab = scipy.io.loadmat('/content/drive/MyDrive/Pt0090_Data_MEGCleaned/NEG_06.mat')\n",
        "negative_run06_F = negative_run06_matlab['NEG_06F']"
      ],
      "metadata": {
        "id": "MeHhVvf6J5GQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Make copies of the loaded data\n",
        "\n",
        "# Run 3 \n",
        "EST_run03_raw_data = copy.deepcopy(EST_run03_F)\n",
        "negative_run03_raw_data = copy.deepcopy(negative_run03_F)\n",
        "print(type(EST_run03_raw_data))\n",
        "print(type(negative_run03_raw_data))\n",
        "\n",
        "# Run 4\n",
        "EST_run04_raw_data = copy.deepcopy(EST_run04_F)\n",
        "negative_run04_raw_data = copy.deepcopy(negative_run04_F)\n",
        "print(type(EST_run04_raw_data))\n",
        "print(type(negative_run04_raw_data))\n",
        "\n",
        "# Run 5 \n",
        "EST_run05_raw_data = copy.deepcopy(EST_run05_F)\n",
        "negative_run05_raw_data = copy.deepcopy(negative_run05_F)\n",
        "print(type(EST_run05_raw_data))\n",
        "print(type(negative_run05_raw_data))\n",
        "\n",
        "# Run 6\n",
        "EST_run06_raw_data = copy.deepcopy(EST_run06_F)\n",
        "negative_run06_raw_data = copy.deepcopy(negative_run06_F)\n",
        "print(type(EST_run06_raw_data))\n",
        "print(type(negative_run06_raw_data))"
      ],
      "metadata": {
        "id": "RrvOsGvgJ7FV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2: Processing the Data for Neural Network"
      ],
      "metadata": {
        "id": "Nz4Bjw1VJ9j7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## We need to transpose raw_data. Currently, the shape of raw_data is [nChannels, nSamples]. In order to use the epoching function \n",
        "## (taken from Raymundo Cassani at https://github.com/MuSAELab/amplitude-modulation-analysis-module/blob/master/am_analysis/am_analysis.py),\n",
        "## we need to transpose raw_data such that its shape becomes [nSamples, nChannels]\n",
        "\n",
        "# Run 3\n",
        "EST_run03_raw_data_transposed = np.transpose(EST_run03_raw_data)\n",
        "print(EST_run03_raw_data_transposed.shape) #(154368, 394)\n",
        "negative_run03_raw_data_transposed = np.transpose(negative_run03_raw_data)\n",
        "print(negative_run03_raw_data_transposed.shape) #(238336, 394)\n",
        "\n",
        "# Run 4\n",
        "EST_run04_raw_data_transposed = np.transpose(EST_run04_raw_data)\n",
        "print(EST_run04_raw_data_transposed.shape) #(60416, 394)\n",
        "negative_run04_raw_data_transposed = np.transpose(negative_run04_raw_data)\n",
        "print(negative_run04_raw_data_transposed.shape) #(57344, 394)\n",
        "\n",
        "# Run 5\n",
        "EST_run05_raw_data_transposed = np.transpose(EST_run05_raw_data)\n",
        "print(EST_run05_raw_data_transposed.shape) #(61952, 394)\n",
        "negative_run05_raw_data_transposed = np.transpose(negative_run05_raw_data)\n",
        "print(negative_run05_raw_data_transposed.shape) #(39936, 394)\n",
        "\n",
        "# Run 6\n",
        "EST_run06_raw_data_transposed = np.transpose(EST_run06_raw_data)\n",
        "print(EST_run06_raw_data_transposed.shape) #(156416, 394)\n",
        "negative_run06_raw_data_transposed = np.transpose(negative_run06_raw_data)\n",
        "print(negative_run06_raw_data_transposed.shape) #(122368, 394)"
      ],
      "metadata": {
        "id": "e3wF-6qNJ-Bn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## By Raymundo Cassani, used with permission from: https://github.com/MuSAELab/amplitude-modulation-analysis-module/blob/master/am_analysis/am_analysis.py\n",
        "\n",
        "def epoching(data, samples_epoch, samples_overlap = 0):\n",
        "    \"\"\"Divide an array in a colletion of smaller arrays\n",
        "    \n",
        "    Divides the `data` provided as [n_samples, n_channels] using the \n",
        "    `size_epoch` indicated (in samples) and the `overlap_epoch` between \n",
        "    consecutive epochs.\n",
        "   \n",
        "    Parameters\n",
        "    ----------\n",
        "    data : 2D array \n",
        "        with shape (n_samples, n_channels)\n",
        "    samples_epochs : \n",
        "        number of samples in smaller epochs\n",
        "        \n",
        "    samples_overlap : \n",
        "        number of samples for ovelap between epochs (Default 0)\n",
        "    Returns\n",
        "    -------\n",
        "    epochs : 3D array \n",
        "        with shape (samples_epoch, n_channels, n_epochs)\n",
        "    \n",
        "    remainder : 2D array \n",
        "        with the remaining data after last complete epoch\n",
        "    \n",
        "    ix_center : 1D array\n",
        "        indicates the index tha corresponds to the center of the nth epoch.\n",
        "    \"\"\" \n",
        "    # input 'data' as 2D matrix [samples, columns]\n",
        "    try:\n",
        "        data.shape[1]\n",
        "    except IndexError:\n",
        "        data = data[:, np.newaxis]\n",
        "    \n",
        "    # number of samples and number of channels\n",
        "    n_samples, n_channels = data.shape\n",
        "\n",
        "    # Size of half epoch\n",
        "    half_epoch = np.ceil(samples_epoch / 2 )\n",
        "\n",
        "    # Epoch shift   \n",
        "    samples_shift = samples_epoch - samples_overlap\n",
        "\n",
        "    # Number of epochs\n",
        "    n_epochs =  int(np.floor( (n_samples - samples_epoch) / float(samples_shift) ) + 1 )\n",
        "    if n_epochs == 0:\n",
        "        return np.array([]), data, np.array([])\n",
        "\n",
        "    #markers indicates where the epoch starts, and the epoch contains samples_epoch rows\n",
        "    markers = np.asarray(range(0,n_epochs)) * samples_shift\n",
        "    markers = markers.astype(int)\n",
        "\n",
        "    #Divide data in epochs\n",
        "    epochs = np.zeros((samples_epoch, n_channels, n_epochs))\n",
        "    ix_center = np.zeros((n_epochs,1))\n",
        "\n",
        "    for i_epoch in range(0,n_epochs):\n",
        "        epochs[:,:,i_epoch] = data[ markers[i_epoch] : markers[i_epoch] + samples_epoch ,:]\n",
        "        ix_center[i_epoch] = markers[i_epoch] -1 + half_epoch\n",
        "        \n",
        "    if ( (markers[-1] + samples_epoch) < n_samples): \n",
        "        remainder = data[markers[-1] + samples_epoch : n_samples, :]\n",
        "    else:\n",
        "        remainder = np.asarray([])\n",
        "    \n",
        "    return epochs, remainder, ix_center.astype(int)"
      ],
      "metadata": {
        "id": "kpwcHCw-KAQk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Now, we need to epoch the positive examples. We will use the epoching function by Raymundo (from \n",
        "## https://github.com/MuSAELab/amplitude-modulation-analysis-module/blob/master/am_analysis/am_analysis.py), taken with permission\n",
        "\n",
        "## epochs_EST_run0# is a numpy ndarray with shape [number of samples, number of channels, number of 1-second epochs]\n",
        "## Note that the number of samples should be 256 since the data in Brainstorm has been downsampled to 256 Hz\n",
        "\n",
        "# Run 3\n",
        "EST_run03_epoched = epoching(EST_run03_raw_data_transposed, 256, 0)\n",
        "epochs_EST_run03 = EST_run03_epoched[0]\n",
        "print(epochs_EST_run03.shape) #(256, 394, 603) \n",
        "negative_run03_epoched = epoching(negative_run03_raw_data_transposed, 256, 0)\n",
        "epochs_negative_run03 = negative_run03_epoched[0]\n",
        "print(epochs_negative_run03.shape) #(256, 394, 931) \n",
        "\n",
        "# Run 4\n",
        "EST_run04_epoched = epoching(EST_run04_raw_data_transposed, 256, 0)\n",
        "epochs_EST_run04 = EST_run04_epoched[0]\n",
        "print(epochs_EST_run04.shape) #(256, 394, 236)\n",
        "negative_run04_epoched = epoching(negative_run04_raw_data_transposed, 256, 0)\n",
        "epochs_negative_run04 = negative_run04_epoched[0]\n",
        "print(epochs_negative_run04.shape) #(256, 394, 224) \n",
        "\n",
        "# Run 5\n",
        "EST_run05_epoched = epoching(EST_run05_raw_data_transposed, 256, 0)\n",
        "epochs_EST_run05 = EST_run05_epoched[0]\n",
        "print(epochs_EST_run05.shape) #(256, 394, 242) \n",
        "negative_run05_epoched = epoching(negative_run05_raw_data_transposed, 256, 0)\n",
        "epochs_negative_run05 = negative_run05_epoched[0]\n",
        "print(epochs_negative_run05.shape) #(256, 394, 156) \n",
        "\n",
        "# Run 6\n",
        "EST_run06_epoched = epoching(EST_run06_raw_data_transposed, 256, 0)\n",
        "epochs_EST_run06 = EST_run06_epoched[0]\n",
        "print(epochs_EST_run06.shape) #(256, 394, 611) \n",
        "negative_run06_epoched = epoching(negative_run06_raw_data_transposed, 256, 0)\n",
        "epochs_negative_run06 = negative_run06_epoched[0]\n",
        "print(epochs_negative_run06.shape) #(256, 394, 478)"
      ],
      "metadata": {
        "id": "yxHIGnvDKA-9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 3: MEG Processing & Modelling"
      ],
      "metadata": {
        "id": "LmGV8EeIKHdz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Now, we don't actually need 394 channels. We only want the MEG ones.\n",
        "## From the channel editor in Brainstorm, we find that channels (1-indexed) 28-301 (274 channels) are MEG channels. So, we need to subset \n",
        "## epochs_EST_run0*. The new shape should be [number of samples, number of channels, number of epochs] where number of samples and\n",
        "## number of epochs stays the same, while number of channels decreases to 274\n",
        "\n",
        "#The M in Mepochs stands for MEG\n",
        "\n",
        "# Run 3\n",
        "Mepochs_EST_run03 = epochs_EST_run03[:,27:301,:]\n",
        "print(\"Mepochs_EST_run03.shape:\", Mepochs_EST_run03.shape) #(256, 274, 603)\n",
        "Mepochs_negative_run03 = epochs_negative_run03[:,27:301,:]\n",
        "print(\"Mepochs_negative_run03.shape:\", Mepochs_negative_run03.shape) #(256, 274, 931)\n",
        "\n",
        "# Run 4\n",
        "Mepochs_EST_run04 = epochs_EST_run04[:,27:301,:]\n",
        "print(\"Mepochs_EST_run04.shape:\", Mepochs_EST_run04.shape) #(256, 274, 236)\n",
        "Mepochs_negative_run04 = epochs_negative_run04[:,27:301,:]\n",
        "print(\"Mepochs_negative_run04.shape:\", Mepochs_negative_run04.shape) #(256, 274, 224)\n",
        "\n",
        "# Run 5\n",
        "Mepochs_EST_run05 = epochs_EST_run05[:,27:301,:]\n",
        "print(\"Mepochs_EST_run05.shape:\", Mepochs_EST_run05.shape) #(256, 274, 242)\n",
        "Mepochs_negative_run05 = epochs_negative_run05[:,27:301,:]\n",
        "print(\"Mepochs_negative_run05.shape:\", Mepochs_negative_run05.shape) #(256, 274, 156)\n",
        "\n",
        "# Run 6\n",
        "Mepochs_EST_run06 = epochs_EST_run06[:,27:301,:]\n",
        "print(\"Mepochs_EST_run06.shape:\", Mepochs_EST_run06.shape) #(256, 274, 611)\n",
        "Mepochs_negative_run06 = epochs_negative_run06[:,27:301,:]\n",
        "print(\"Mepochs_negative_run06.shape:\", Mepochs_negative_run06.shape) #(256, 274, 478)"
      ],
      "metadata": {
        "id": "H5gkhpDnKH0L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Transpose to be (number of 1-second epochs, number of samples, number of channels)\n",
        "\n",
        "MEG_EST_run03 = Mepochs_EST_run03.transpose(2, 0, 1) \n",
        "MEG_EST_run04 = Mepochs_EST_run04.transpose(2, 0, 1)\n",
        "MEG_EST_run05 = Mepochs_EST_run05.transpose(2, 0, 1)\n",
        "MEG_EST_run06 = Mepochs_EST_run06.transpose(2, 0, 1)\n",
        "print(MEG_EST_run03.shape) #(603, 256, 274)\n",
        "print(MEG_EST_run04.shape) #(236, 256, 274)\n",
        "print(MEG_EST_run05.shape) #(242, 256, 274)\n",
        "print(MEG_EST_run06.shape) #(611, 256, 274)\n",
        "\n",
        "MEG_negative_run03 = Mepochs_negative_run03.transpose(2,0,1)\n",
        "MEG_negative_run04 = Mepochs_negative_run04.transpose(2,0,1)\n",
        "MEG_negative_run05 = Mepochs_negative_run05.transpose(2,0,1)\n",
        "MEG_negative_run06 = Mepochs_negative_run06.transpose(2,0,1)\n",
        "print(MEG_negative_run03.shape) #(931, 256, 274)\n",
        "print(MEG_negative_run04.shape) #(224, 256, 274)\n",
        "print(MEG_negative_run05.shape) #(156, 256, 274)\n",
        "print(MEG_negative_run06.shape) #(478, 256, 274)"
      ],
      "metadata": {
        "id": "-_Q4B7OWKJ0B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## During preprocessing in Brainstorm, we identified the following channels to be bad channels. So, we need to remove them from the data structures above.\n",
        "## MRT51 284-28=256\n",
        "## MLT55 157-28=129\n",
        "## MRT41 277-28=249\n",
        "## MLT47 152-28=124\n",
        "## MLT57 159-28=131\n",
        "## MLT56 158-28=130\n",
        "## MRT42 278-28=250\n",
        "\n",
        "MEG_EST_run03= np.delete(MEG_EST_run03, [129, 256, 249, 124, 131, 130, 250], axis=2)\n",
        "print(MEG_EST_run03.shape) #(603, 256, 267)\n",
        "MEG_EST_run04= np.delete(MEG_EST_run04, [129, 256, 249, 124, 131, 130, 250], axis=2)\n",
        "print(MEG_EST_run04.shape) #(236, 256, 267)\n",
        "MEG_EST_run05= np.delete(MEG_EST_run05, [129, 256, 249, 124, 131, 130, 250], axis=2)\n",
        "print(MEG_EST_run05.shape) #(242, 256, 267)\n",
        "MEG_EST_run06= np.delete(MEG_EST_run06, [129, 256, 249, 124, 131, 130, 250], axis=2)\n",
        "print(MEG_EST_run06.shape) #(611, 256, 267)\n",
        "\n",
        "MEG_negative_run03 = np.delete(MEG_negative_run03, [129, 256, 249, 124, 131, 130, 250], axis=2)\n",
        "print(MEG_negative_run03.shape) #(931, 256, 267)\n",
        "MEG_negative_run04 = np.delete(MEG_negative_run04, [129, 256, 249, 124, 131, 130, 250], axis=2)\n",
        "print(MEG_negative_run04.shape) #(224, 256, 267)\n",
        "MEG_negative_run05 = np.delete(MEG_negative_run05, [129, 256, 249, 124, 131, 130, 250], axis=2)\n",
        "print(MEG_negative_run05.shape) #(156, 256, 267)\n",
        "MEG_negative_run06 = np.delete(MEG_negative_run06, [129, 256, 249, 124, 131, 130, 250], axis=2)\n",
        "print(MEG_negative_run06.shape) #(478, 256, 267)\n"
      ],
      "metadata": {
        "id": "1OI0nSCnr3FT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Visualization of Averages:\n",
        "\n"
      ],
      "metadata": {
        "id": "H7DJqen6j9tb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a function to display the MEG time series data.\n",
        "\n",
        "def show_time_series(data, xlabel, ylabel):\n",
        "  cur = data \n",
        "  plt.plot(cur)\n",
        "  plt.gca().invert_yaxis() #Invert to match the default reversed y-axis view in Brainstorm.\n",
        "  plt.xlabel(xlabel)\n",
        "  plt.ylabel(ylabel)\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "ILbDNYRGkApc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Look at the average of positive samples:\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "q9XwzWejmTtN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We want to average across the first axis\n",
        "\n",
        "average_MEG_EST_run03 = np.mean(MEG_EST_run03, axis=0)\n",
        "show_time_series(average_MEG_EST_run03, \"Sample number\", \"Teslas\")"
      ],
      "metadata": {
        "id": "cFb8NRI2kTpW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We want to average across the first axis\n",
        "\n",
        "average_MEG_EST_run04 = np.mean(MEG_EST_run04, axis=0)\n",
        "show_time_series(average_MEG_EST_run04, \"Sample number\", \"Teslas\")"
      ],
      "metadata": {
        "id": "cUavpbcZZsqw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We want to average across the first axis\n",
        "\n",
        "average_MEG_EST_run05 = np.mean(MEG_EST_run05, axis=0)\n",
        "show_time_series(average_MEG_EST_run05, \"Sample number\", \"Teslas\")"
      ],
      "metadata": {
        "id": "yQfewracV7jN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We want to average across the first axis\n",
        "\n",
        "average_MEG_EST_run06 = np.mean(MEG_EST_run06, axis=0)\n",
        "show_time_series(average_MEG_EST_run06, \"Sample number\", \"Teslas\")"
      ],
      "metadata": {
        "id": "JKFtwO8FZ0bl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "average_MEG_negative_run03 = np.mean(MEG_negative_run03, axis=0)\n",
        "show_time_series(average_MEG_negative_run03, \"Sample number\", \"Teslas\")"
      ],
      "metadata": {
        "id": "UwsAMYyZWNQW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "average_MEG_negative_run04 = np.mean(MEG_negative_run04, axis=0)\n",
        "show_time_series(average_MEG_negative_run04, \"Sample number\", \"Teslas\")"
      ],
      "metadata": {
        "id": "WpSIFFglaO4Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "average_MEG_negative_run05 = np.mean(MEG_negative_run05, axis=0)\n",
        "show_time_series(average_MEG_negative_run05, \"Sample number\", \"Teslas\")"
      ],
      "metadata": {
        "id": "2-g8PFWfaeXO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "average_MEG_negative_run06 = np.mean(MEG_negative_run06, axis=0)\n",
        "show_time_series(average_MEG_negative_run06, \"Sample number\", \"Teslas\")"
      ],
      "metadata": {
        "id": "bQKZ-YRwah8l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Now, scale the data:\n",
        "\n",
        "def scale_data(data):\n",
        "  counter = 0\n",
        "  return_copy = copy.deepcopy(data)\n",
        "  for item in data:\n",
        "    one_column = np.reshape(item, (-1, 1))\n",
        "    scaler = MinMaxScaler()\n",
        "    scaler.fit(one_column)\n",
        "    one_column = scaler.transform(one_column)\n",
        "    transformed = np.reshape(one_column, (data.shape[1], data.shape[2]))\n",
        "    return_copy[counter] = transformed\n",
        "    counter=counter+1\n",
        "  \n",
        "  return return_copy\n",
        "\n",
        "MEG_EST_run03_scaled = scale_data(MEG_EST_run03)\n",
        "MEG_EST_run04_scaled = scale_data(MEG_EST_run04)\n",
        "MEG_EST_run05_scaled = scale_data(MEG_EST_run05)\n",
        "MEG_EST_run06_scaled = scale_data(MEG_EST_run06)\n",
        "\n",
        "MEG_negative_run03_scaled = scale_data(MEG_negative_run03)\n",
        "MEG_negative_run04_scaled = scale_data(MEG_negative_run04)\n",
        "MEG_negative_run05_scaled = scale_data(MEG_negative_run05)\n",
        "MEG_negative_run06_scaled = scale_data(MEG_negative_run06)\n"
      ],
      "metadata": {
        "id": "qtXkqGTFkT9X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the average of the scaled data\n",
        "\n",
        "average_MEG_EST_run03_scaled = np.mean(MEG_EST_run03_scaled, axis=0)\n",
        "show_time_series(average_MEG_EST_run03_scaled, \"Sample number\", \"Amplitude\")"
      ],
      "metadata": {
        "id": "vBS9kE5m0S9L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the average of the scaled data\n",
        "\n",
        "average_MEG_EST_run04_scaled = np.mean(MEG_EST_run04_scaled, axis=0)\n",
        "show_time_series(average_MEG_EST_run04_scaled,\"Sample number\", \"Amplitude\")"
      ],
      "metadata": {
        "id": "seexR1jXq0uM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the average of the scaled data\n",
        "\n",
        "average_MEG_EST_run05_scaled = np.mean(MEG_EST_run05_scaled, axis=0)\n",
        "show_time_series(average_MEG_EST_run05_scaled, \"Sample number\", \"Amplitude\")"
      ],
      "metadata": {
        "id": "Udhrr78iq3VJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the average of the scaled data\n",
        "\n",
        "average_MEG_EST_run06_scaled = np.mean(MEG_EST_run06_scaled, axis=0)\n",
        "show_time_series(average_MEG_EST_run06_scaled, \"Sample number\", \"Amplitude\")"
      ],
      "metadata": {
        "id": "coJYDUI0q6C2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the average of the scaled data\n",
        "\n",
        "average_MEG_negative_run03_scaled = np.mean(MEG_negative_run03_scaled, axis=0)\n",
        "show_time_series(average_MEG_negative_run03_scaled, \"Sample number\", \"Amplitude\")"
      ],
      "metadata": {
        "id": "pC5ZG14FHn9E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the average of the scaled data\n",
        "\n",
        "average_MEG_negative_run04_scaled = np.mean(MEG_negative_run04_scaled, axis=0)\n",
        "show_time_series(average_MEG_negative_run04_scaled, \"Sample number\", \"Amplitude\")"
      ],
      "metadata": {
        "id": "JPyYNWpjXXar"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the average of the scaled data\n",
        "\n",
        "average_MEG_negative_run05_scaled = np.mean(MEG_negative_run05_scaled, axis=0)\n",
        "show_time_series(average_MEG_negative_run05_scaled, \"Sample number\", \"Amplitude\")"
      ],
      "metadata": {
        "id": "WBpuHLh8rARk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the average of the scaled data\n",
        "\n",
        "average_MEG_negative_run06_scaled = np.mean(MEG_negative_run06_scaled, axis=0)\n",
        "show_time_series(average_MEG_negative_run06_scaled, \"Sample number\", \"Amplitude\")"
      ],
      "metadata": {
        "id": "aS3r1PqTrDOF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Again, we can see that there is a difference between the IED and the non-IED samples! Now, we can move on to training the CNN with our data."
      ],
      "metadata": {
        "id": "39I3v0uoLLV9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 4: Hyperparameter Grid Search with Cross Validation"
      ],
      "metadata": {
        "id": "_i4aBuYfxgjw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Create training and validation data\n",
        "\n",
        "MEG_pos = np.concatenate((MEG_EST_run03_scaled, MEG_EST_run04_scaled, MEG_EST_run05_scaled, MEG_EST_run06_scaled))\n",
        "MEG_neg = np.concatenate((MEG_negative_run03_scaled, MEG_negative_run04_scaled, MEG_negative_run05_scaled, MEG_negative_run06_scaled))\n",
        "MEG_all_X = np.concatenate((MEG_pos, MEG_neg))\n",
        "print(MEG_all_X.shape) #(3481, 256, 267)\n",
        "\n",
        "MEG_y_1s = np.ones(MEG_pos.shape[0]).astype(int)\n",
        "MEG_y_0s = np.zeros(MEG_neg.shape[0]).astype(int)\n",
        "MEG_all_y = np.concatenate((MEG_y_1s, MEG_y_0s))\n",
        "print(MEG_all_y.shape) #(3481, )"
      ],
      "metadata": {
        "id": "ai7G7136KLhq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the train_test_split function to easily shuffle the data and obtain the training set.\n",
        "\n",
        "MEG_X_grid_train,MEG_X_grid_val,MEG_y_grid_train,MEG_y_grid_val = train_test_split(MEG_all_X,MEG_all_y, test_size=0.01)\n",
        "print(MEG_X_grid_train.shape)\n",
        "print(MEG_y_grid_train.shape)"
      ],
      "metadata": {
        "id": "LAQme6rULCuJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print a few y's to make sure that the data is indeed shuffled:\n",
        "\n",
        "print(MEG_y_grid_train[0:20])"
      ],
      "metadata": {
        "id": "D9-R1YYoL1vv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  from scikeras.wrappers import KerasClassifier\n",
        "except:\n",
        "  !pip install scikeras\n",
        "  from scikeras.wrappers import KerasClassifier\n",
        "from sklearn.model_selection import GridSearchCV"
      ],
      "metadata": {
        "id": "EezjAn_GJzfD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_clf(dropout, filters, fullyneurons):\n",
        "  # create model\n",
        "  model = tf.keras.Sequential() #Create a sequential model\n",
        "\n",
        "  #Convolutional layer #1\n",
        "  #Padding set to same to preserve the spatial dimensions of the volume such that the output volume size matches the input volume size\n",
        "  model.add(layers.Conv2D(filters, (3, 3), padding=\"same\", input_shape=(256, 267, 1))) \n",
        "  model.add(layers.BatchNormalization())\n",
        "  model.add(layers.Activation(\"relu\"))\n",
        "  model.add(layers.MaxPool2D(pool_size=(2,2)))\n",
        "  model.add(layers.Dropout(dropout)) \n",
        "\n",
        "  #Convolutional layer #2\n",
        "  model.add(layers.Conv2D(filters*2, (3, 3), padding=\"same\")) \n",
        "  model.add(layers.BatchNormalization())\n",
        "  model.add(layers.Activation(\"relu\"))\n",
        "  model.add(layers.MaxPool2D(pool_size=(2,2))) \n",
        "  model.add(layers.Dropout(dropout))\n",
        "\n",
        "  #Convolutional layer #3\n",
        "  model.add(layers.Conv2D(filters*3, (3, 3), padding=\"same\")) \n",
        "  model.add(layers.BatchNormalization())\n",
        "  model.add(layers.Activation(\"relu\"))\n",
        "  model.add(layers.MaxPool2D(pool_size=(2,2))) \n",
        "  model.add(layers.Dropout(dropout)) \n",
        "\n",
        "  #Flatten the volume to pass through fully connected layer\n",
        "  model.add(layers.Flatten())\n",
        "\n",
        "  #Fully connected layer #1\n",
        "  model.add(layers.Dense(fullyneurons))\n",
        "  model.add(layers.BatchNormalization())\n",
        "  model.add(layers.Activation(\"relu\"))\n",
        "  model.add(layers.Dropout(dropout))\n",
        "\n",
        "  #Classification result\n",
        "  model.add(layers.Dense(1, activation=\"sigmoid\")) #2 possible output: epileptiform, or non-epileptiform. \n",
        "  return model"
      ],
      "metadata": {
        "id": "MkK5F9I0J5ih"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reduce_lr = ReduceLROnPlateau(monitor=\"loss\", factor=0.1, patience=2, min_lr=0.00001, model=\"auto\") #learning rate scheduling, decreases learning rate by 0.1 if doesn't get better for 2 epochs\n",
        "\n",
        "early_stop = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10) #stop training once val loss stops decreasing for 10 passes\n",
        "\n",
        "cb = [reduce_lr, early_stop]\n",
        "\n",
        "clf = KerasClassifier(\n",
        "    model=get_clf,\n",
        "    loss=\"binary_crossentropy\",\n",
        "    optimizer=Adam(learning_rate=0.01, beta_1=0.9, beta_2=0.999, epsilon=1e-08),\n",
        "    model__dropout=0.2,\n",
        "    model__filters = 8, \n",
        "    model__fullyneurons = 50,\n",
        "    fit__validation_split=0.2,\n",
        "    verbose=False,\n",
        "    epochs=100, #maximum 100 passes\n",
        "    callbacks=cb,\n",
        "    metrics=[\"accuracy\", tf.keras.metrics.AUC(curve=\"ROC\", from_logits=True), tf.keras.metrics.AUC(curve=\"PR\", from_logits=True), tf.keras.metrics.Precision(), tf.keras.metrics.Recall()]\n",
        ")"
      ],
      "metadata": {
        "id": "QlgSwKAJKVeT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn.metrics\n",
        "\n",
        "scoring_metrics = ['accuracy','roc_auc']\n",
        "\n",
        "params = {\n",
        "    'model__dropout': [0.2, 0.4],\n",
        "    'model__filters': [8, 16, 32],\n",
        "    'model__fullyneurons': [50, 100, 500],\n",
        "}\n",
        "\n",
        "gs = GridSearchCV(clf, params, scoring=scoring_metrics, refit='accuracy', n_jobs=1, verbose=3, cv=3)\n",
        "\n",
        "gs.fit(MEG_X_grid_train, MEG_y_grid_train)\n",
        "\n",
        "print(gs.best_score_, gs.best_params_)"
      ],
      "metadata": {
        "id": "IAMiTU1zK0kJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "gs_dict = gs.cv_results_\n",
        "del gs_dict['mean_fit_time']\n",
        "del gs_dict['std_fit_time']\n",
        "del gs_dict['mean_score_time']\n",
        "del gs_dict['std_score_time']\n",
        "del gs_dict['params']\n",
        "gs_df = pd.DataFrame.from_dict(gs_dict)\n",
        "gs_df.to_csv(\"/content/drive/MyDrive/Final_IED_Data/meg_gs_df_edited.csv\") "
      ],
      "metadata": {
        "id": "t_NXSjqBMLGf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 5: Training and Testing Model Using Optimal Hyperparameters"
      ],
      "metadata": {
        "id": "867Vs-2LJqQ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Training, Validation, and Test Sets from all of our available data\n",
        "# Use 70/15/15 split since dataset is relatively small\n",
        "\n",
        "MEG_X_train,MEG_X_val_test,MEG_y_train,MEG_y_val_test = train_test_split(MEG_all_X,MEG_all_y, test_size=0.3)\n",
        "MEG_X_val,MEG_X_test,MEG_y_val,MEG_y_test = train_test_split(MEG_X_val_test,MEG_y_val_test, test_size=0.5)\n",
        "\n",
        "print(MEG_X_train.shape)\n",
        "print(MEG_y_train.shape)\n",
        "print(MEG_X_val.shape)\n",
        "print(MEG_y_val.shape)\n",
        "print(MEG_X_test.shape)\n",
        "print(MEG_y_test.shape)\n",
        "\n",
        "# Print a few y's to make sure that the data is indeed shuffled:\n",
        "\n",
        "print(MEG_y_test[0:10])\n",
        "print(MEG_y_val[0:10])\n",
        "print(MEG_y_train[0:10])"
      ],
      "metadata": {
        "id": "myCIG9WIKMyZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pickle.dump(MEG_X_train, open(\"/content/drive/MyDrive/Final_IED_Data/MEG_X_train.pickle\",'wb'))\n",
        "pickle.dump(MEG_X_val, open(\"/content/drive/MyDrive/Final_IED_Data/MEG_X_val.pickle\",'wb'))\n",
        "pickle.dump(MEG_X_test, open(\"/content/drive/MyDrive/Final_IED_Data/MEG_X_test.pickle\",'wb'))\n",
        "pickle.dump(MEG_y_train, open(\"/content/drive/MyDrive/Final_IED_Data/MEG_y_train.pickle\",'wb'))\n",
        "pickle.dump(MEG_y_val, open(\"/content/drive/MyDrive/Final_IED_Data/MEG_y_val.pickle\",'wb'))\n",
        "pickle.dump(MEG_y_test, open(\"/content/drive/MyDrive/Final_IED_Data/MEG_y_test.pickle\",'wb'))"
      ],
      "metadata": {
        "id": "4XQdlvjH2NnU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.Sequential() #Create a sequential model\n",
        "\n",
        "#Convolutional layer #1\n",
        "#Padding set to same to preserve the spatial dimensions of the volume such that the output volume size matches the input volume size\n",
        "model.add(layers.Conv2D(8, (3, 3), padding=\"same\", input_shape=(256, 267, 1))) \n",
        "model.add(layers.BatchNormalization())\n",
        "model.add(layers.Activation(\"relu\"))\n",
        "model.add(layers.MaxPool2D(pool_size=(2,2))) #Max pooling used to reduce the spatial dimensions of the output volume.\n",
        "model.add(layers.Dropout(0.20)) #Prevents overfitting\n",
        "\n",
        "#Convolutional layer #2\n",
        "model.add(layers.Conv2D(16, (3, 3), padding=\"same\"))\n",
        "model.add(layers.BatchNormalization())\n",
        "model.add(layers.Activation(\"relu\"))\n",
        "model.add(layers.MaxPool2D(pool_size=(2,2))) #Max pooling used to reduce the spatial dimensions of the output volume.\n",
        "model.add(layers.Dropout(0.20)) #Prevents overfitting\n",
        "\n",
        "#Convolutional layer #3\n",
        "model.add(layers.Conv2D(32, (3, 3), padding=\"same\"))\n",
        "model.add(layers.BatchNormalization())\n",
        "model.add(layers.Activation(\"relu\"))\n",
        "model.add(layers.MaxPool2D(pool_size=(2,2))) #Max pooling used to reduce the spatial dimensions of the output volume.\n",
        "model.add(layers.Dropout(0.20)) #Prevents overfitting\n",
        "\n",
        "#Flatten the volume to pass through fully connected layer\n",
        "model.add(layers.Flatten())\n",
        "\n",
        "#Fully connected layer #1\n",
        "model.add(layers.Dense(50))\n",
        "model.add(layers.BatchNormalization())\n",
        "model.add(layers.Activation(\"relu\"))\n",
        "model.add(layers.Dropout(0.25)) #Prevents overfitting\n",
        "\n",
        "#Classification result\n",
        "model.add(layers.Dense(1, activation=\"sigmoid\")) #2 possible output: epileptiform, or non-epileptiform.\n",
        "\n",
        "#We will use the Adam optimizer, and compile our model\n",
        "optimizer = Adam(learning_rate=0.01, beta_1=0.9, beta_2=0.999, epsilon=1e-08) \n",
        "model.compile(optimizer=optimizer, loss=tf.keras.losses.BinaryCrossentropy(), metrics=[\"accuracy\", tf.keras.metrics.AUC(curve=\"ROC\", from_logits=True), tf.keras.metrics.AUC(curve=\"PR\", from_logits=True), tf.keras.metrics.Precision(), tf.keras.metrics.Recall()]) #Loss set to binary_crossentropy using Boston paper #Note to self: labels need to be in one-hot representation"
      ],
      "metadata": {
        "id": "XZlkyPPEKPN2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "keras.backend.clear_session()"
      ],
      "metadata": {
        "id": "HHZTgexlKRav"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 20\n",
        "\n",
        "checkpoint = ModelCheckpoint(\"model_weights.h5\", monitor=\"val_accuracy\", \n",
        "                            save_weights_only=True, mode=\"max\", verbose=1)\n",
        "\n",
        "reduce_lr = ReduceLROnPlateau(monitor=\"loss\", factor=0.1, patience=2, min_lr=0.00001, model=\"auto\") #learning rate scheduling, decreases learning rate by 0.1 if doesn't get better for 2 epochs\n",
        "\n",
        "callbacks = [checkpoint, reduce_lr]\n",
        "\n",
        "history = model.fit(\n",
        "    x = MEG_X_train,\n",
        "    y = MEG_y_train,\n",
        "    validation_data = (MEG_X_val, MEG_y_val),\n",
        "    epochs=epochs,\n",
        "    callbacks = callbacks\n",
        "    )"
      ],
      "metadata": {
        "id": "3E7n6qfgKT4k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = model.evaluate(MEG_X_test, MEG_y_test)"
      ],
      "metadata": {
        "id": "EV8oopZzKtRj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save my model to my Google Drive\n",
        "\n",
        "model.save(\"/content/drive/MyDrive/Final IED Detectors/Final_MEG_Model\")"
      ],
      "metadata": {
        "id": "LXJkyPCbjGAF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use this to load the trained model\n",
        "reconstructed_model = keras.models.load_model(\"/content/drive/MyDrive/Final IED Detectors/Final_MEG_Model\")"
      ],
      "metadata": {
        "id": "ASZIiwOQnoiL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MEG_X_train = pickle.load(open(\"/content/drive/MyDrive/Final_IED_Data/MEG_X_train.pickle\",'rb'))\n",
        "MEG_X_val = pickle.load(open(\"/content/drive/MyDrive/Final_IED_Data/MEG_X_val.pickle\",'rb'))\n",
        "MEG_X_test = pickle.load(open(\"/content/drive/MyDrive/Final_IED_Data/MEG_X_test.pickle\",'rb'))\n",
        "MEG_y_train = pickle.load(open(\"/content/drive/MyDrive/Final_IED_Data/MEG_y_train.pickle\",'rb'))\n",
        "MEG_y_val = pickle.load(open(\"/content/drive/MyDrive/Final_IED_Data/MEG_y_val.pickle\",'rb'))\n",
        "MEG_y_test = pickle.load(open(\"/content/drive/MyDrive/Final_IED_Data/MEG_y_test.pickle\",'rb'))"
      ],
      "metadata": {
        "id": "NnkCPHZpikK8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results_reconstructed = reconstructed_model.evaluate(MEG_X_test, MEG_y_test)"
      ],
      "metadata": {
        "id": "YslitusX-A1w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 6: Visualize the Model's Classication"
      ],
      "metadata": {
        "id": "5xlSkqE3jLkU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prediction_test = model.predict(MEG_X_test)\n",
        "prediction_test = (prediction_test > 0.5).astype(np.float32)\n",
        "prediction_test = prediction_test.flatten()"
      ],
      "metadata": {
        "id": "Bw_PpbV1LoYJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "positive_indices = []\n",
        "negative_indices = []\n",
        "\n",
        "for i in range(0, len(prediction_test)):\n",
        "  if prediction_test[i] == 1:\n",
        "    positive_indices.append(i)\n",
        "  else:\n",
        "    negative_indices.append(i)"
      ],
      "metadata": {
        "id": "GF8-k4PUj70G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_test_positives = np.mean(MEG_X_test[positive_indices, :, :], axis=0)\n",
        "show_time_series(predicted_test_positives, \"Sample number\", \"Amplitude\")"
      ],
      "metadata": {
        "id": "j1r2nNfwj76V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_test_negatives = np.mean(MEG_X_test[negative_indices, :, :], axis=0)\n",
        "show_time_series(predicted_test_negatives, \"Sample number\", \"Amplitude\")"
      ],
      "metadata": {
        "id": "jfzHOOILj78a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Stress testing the model on only the positive data in the test dataset\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "OW20eu3wkfnf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ied_only_indices = []\n",
        "\n",
        "for i in range(0, len(MEG_y_test)):\n",
        "  if MEG_y_test[i] == 1:\n",
        "    ied_only_indices.append(i)"
      ],
      "metadata": {
        "id": "0jbXfs4L3Ljo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "IED_only_prediction = model.predict(MEG_X_test[ied_only_indices,:,:])\n",
        "IED_only_prediction = (IED_only_prediction > 0.5).astype(np.float32)\n",
        "IED_only_prediction = IED_only_prediction.flatten()"
      ],
      "metadata": {
        "id": "Gp2xMeGq3sCg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "collections.Counter(IED_only_prediction)"
      ],
      "metadata": {
        "id": "KPwHMXtfJZV5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Stress testing the model on only the negative data in the test dataset"
      ],
      "metadata": {
        "id": "671UHTWZkjIO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "neg_only_indices = []\n",
        "\n",
        "for i in range(0, len(MEG_y_test)):\n",
        "  if MEG_y_test[i] == 0:\n",
        "    neg_only_indices.append(i)"
      ],
      "metadata": {
        "id": "opZrkec_ke6_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "neg_only_prediction = model.predict(MEG_X_test[neg_only_indices,:,:])\n",
        "neg_only_prediction = (neg_only_prediction > 0.5).astype(np.float32)\n",
        "neg_only_prediction = neg_only_prediction.flatten()"
      ],
      "metadata": {
        "id": "ggoFnE8g4MCq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "collections.Counter(neg_only_prediction)"
      ],
      "metadata": {
        "id": "EHLxP17fluzP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}